{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe3849e4",
   "metadata": {
    "id": "fe3849e4"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from scipy import optimize\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import cv2 # this is for image feature extraction SIFT and HoG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2788c573",
   "metadata": {},
   "source": [
    "#### pre-processing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecb8e7e6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 600
    },
    "id": "ecb8e7e6",
    "outputId": "9708a831-f728-45bf-8f32-70d1ab2d5c87"
   },
   "outputs": [],
   "source": [
    "def to_image(img): # takes an flattened 3072 image and transforms it into 3x32x32 RGB image\n",
    "    X = np.moveaxis(img.reshape(3, 32, 32), 0, -1)\n",
    "    return (X - np.min(X)) /(np.max(X) - np.min(X))\n",
    "\n",
    "def to_gray(img): # takes a RGB image and transforms it into grayscale\n",
    "    X = np.moveaxis(img.reshape(3, 32, 32), 0, -1)\n",
    "    gray = 0.299 * X[:,:,0] + 0.587 * X[:,:,1] + 0.114* X[:,:,2]\n",
    "    return (gray - np.min(gray)) /(np.max(gray) - np.min(gray)) # we normalize them\n",
    "\n",
    "def visualize(imgs, nb_cols = 10): # code to visualize the images\n",
    "    N = imgs.shape[0]\n",
    "    n = N//nb_cols\n",
    "    fig, axes = plt.subplots(n, nb_cols, figsize=(nb_cols * 1.5, n * 1.5))\n",
    "    for i in range(n):\n",
    "        for j in range(nb_cols):\n",
    "            axes[i, j].imshow(to_image(imgs[i*nb_cols+j, :]))\n",
    "            axes[i, j].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "def flip(x): # x is a flattened version, flip returns the flipped image (the symmetrical along the x_axis)\n",
    "    # the flipped image should have the same label, we use this for data augmentation\n",
    "    img = np.moveaxis(x.reshape(3,32,32),0,-1)\n",
    "    img_flipped = np.zeros(img.shape)\n",
    "    for i in range(3) :\n",
    "        img_flipped[:,:,i] = img[:,::-1,i]\n",
    "\n",
    "    return np.ravel(np.moveaxis(img_flipped, -1, 0)) # we return the flattened version\n",
    "\n",
    "def augment_data(imgs, y): # we augment a dataset if images with a set of the flipped images\n",
    "    return np.concatenate([imgs, np.apply_along_axis(flip, axis=1, arr=imgs)]), np.concatenate([y, y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c8f4a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StandardScaler: # implementation of StandardScaler\n",
    "    def __init__(self):\n",
    "        self.mean_ = None\n",
    "        self.scale_ = None\n",
    "    def fit_transform(self, X):\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        self.scale_ = np.std(X, axis=0)\n",
    "        self.scale_[self.scale_ == 0] = 1\n",
    "        return (X - self.mean_) / self.scale_\n",
    "    def transform(self, X):\n",
    "        return (X - self.mean_) / self.scale_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543a7f60",
   "metadata": {},
   "source": [
    "#### kernels :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "426d5eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RBF:\n",
    "    def __init__(self, sigma=1.):\n",
    "        self.sigma = sigma  ## the variance of the kernel\n",
    "        self.name = 'RBF'\n",
    "    def kernel(self,X,Y):\n",
    "        squared_norm = np.expand_dims(np.sum(X**2,axis=1),axis=1) + np.expand_dims(np.sum(Y**2,axis=1),axis=0)-2*np.einsum('ni,mi->nm',X,Y)\n",
    "        return np.exp(-0.5*squared_norm/self.sigma**2)\n",
    "    \n",
    "class RBF_batches: # the RBF kernel but in batches to help reduce the space (memory) complexity\n",
    "    def __init__(self, sigma=1., batch_size=10):\n",
    "        self.sigma = sigma  ## the variance of the kernel\n",
    "        self.batch_size = batch_size\n",
    "    def kernel(self, X, Y):\n",
    "        ## Input vectors X and Y of shape Nxd and Mxd\n",
    "        N = X.shape[0]\n",
    "        M = Y.shape[0]\n",
    "        res = np.zeros((N, M))\n",
    "        X_nbbatches = N//self.batch_size\n",
    "        Y_nbbatches = M//self.batch_size\n",
    "        for i in range(X_nbbatches):\n",
    "            for j in range(Y_nbbatches):\n",
    "                X_batch = X[i*self.batch_size:(i+1)*self.batch_size, :]\n",
    "                Y_batch = Y[j*self.batch_size:(j+1)*self.batch_size, :]\n",
    "                squared_norm = np.expand_dims(np.sum(X_batch**2,axis=1),axis=1) + np.expand_dims(np.sum(Y_batch**2,axis=1),axis=0)-2*np.einsum('ni,mi->nm',X_batch,Y_batch)\n",
    "                res[i*self.batch_size:(i+1)*self.batch_size, j*self.batch_size:(j+1)*self.batch_size] = np.exp(- 0.5 * squared_norm / (self.sigma**2))\n",
    "        return res\n",
    "\n",
    "class Polynomial: # the polynomial kernel includes the linear kernel for d=1\n",
    "    def __init__(self, d=1.):\n",
    "        self.d = d ## the variance of the kernel\n",
    "        self.type = 'poly'\n",
    "\n",
    "    def kernel(self, X, Y):\n",
    "        ## Input vectors X and Y of shape Nxd and Mxd\n",
    "        return (X @ Y.T)**self.d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4b74597",
   "metadata": {},
   "source": [
    "#### kernel methods :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "287852a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KernelPCA: # kernel pca will be used to reduce the dimension of the dataset\n",
    "    def __init__(self,kernel, r=2):\n",
    "        self.kernel = kernel          # <---\n",
    "        self.alpha = None # Matrix of shape N times d representing the d eingenvectors alpha corresp\n",
    "        self.lmbda = None # Vector of size d representing the top d eingenvalues\n",
    "        self.support = None # Data points where the features are evaluated\n",
    "        self.r =r ## Number of principal components\n",
    "    def compute_PCA(self, X):\n",
    "        # assigns the vectors\n",
    "        N = X.shape[0]\n",
    "        K = self.kernel(X, X)\n",
    "        J = np.ones((N, N))\n",
    "        I = np.eye(N)\n",
    "\n",
    "        G = (I - J/N) @ K @ (I - J/N)\n",
    "        self.support = X\n",
    "        eigen_values, eigen_vectors = np.linalg.eig(G)\n",
    "        idx = np.argsort(eigen_values)\n",
    "        self.lmbda = eigen_values[idx][-self.r:].real\n",
    "        self.alpha = eigen_vectors[:, idx][:, -self.r:].real / np.sqrt(N * self.lmbda).reshape(1, self.r)\n",
    "\n",
    "    def transform(self,x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        N = self.support.shape[0]\n",
    "        J = np.ones((N, N))\n",
    "        I = np.eye(N)\n",
    "        k = self.kernel(x, self.support)\n",
    "        g = k @ (I - J/N)\n",
    "        return g @ self.alpha\n",
    "    \n",
    "class KLR: # kernel binary logistic regression\n",
    "    def __init__(self, lbda, max_iters=100):\n",
    "        self.lbda = lbda\n",
    "        self.max_iters = max_iters\n",
    "        self.weights = None\n",
    "    def fit(self, X, y, K):\n",
    "        N = len(y)\n",
    "        def loss(w):\n",
    "            logistic = np.mean(np.log(1 + np.exp(-y*(K@w)))) + 0.5 * self.lbda * np.dot(w, K @ w)\n",
    "            return logistic\n",
    "        def grad_loss(w):\n",
    "            P = - 1 / (1 + np.exp(y*(K@w)))\n",
    "            return (1/N) * K @ (P*y) + self.lbda * (K@w)\n",
    "        res = optimize.minimize(loss, x0=np.ones(N), jac=grad_loss, method='L-BFGS-B', options={'maxiter': self.max_iters})\n",
    "        self.weights = res.x\n",
    "    def predict(self, x, K):\n",
    "        z = 1 / (1 + np.exp(-(K @ self.weights)))\n",
    "        return np.where(z>0.5, 1, -1)\n",
    "\n",
    "class KernelSVC: # simple binary classification SVC\n",
    "\n",
    "    def __init__(self, C, kernel, epsilon = 1e-3):\n",
    "        self.type = 'non-linear'\n",
    "        self.C = C\n",
    "        self.alpha = None\n",
    "        self.support = None\n",
    "        self.epsilon = epsilon\n",
    "        self.kernel = kernel\n",
    "        self.X_train = None\n",
    "        self.to_fit = True\n",
    "\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        #### You might define here any variable needed for the rest of the code\n",
    "        N = len(y)\n",
    "        # Lagrange dual problem\n",
    "        K = self.kernel(X, X)\n",
    "        G = np.dot(np.diag(y), np.dot(K, np.diag(y)))\n",
    "        def loss(alpha):\n",
    "            return - np.sum(alpha) + 0.5 * np.dot(alpha, np.dot(G, alpha))\n",
    "\n",
    "        # Partial derivate of Ld on alpha\n",
    "        def grad_loss(alpha):\n",
    "            return - np.ones(N) + np.dot(G, alpha)\n",
    "\n",
    "\n",
    "        # Constraints on alpha of the shape :\n",
    "        # -  d - C*alpha  = 0\n",
    "        # -  b - A*alpha >= 0\n",
    "\n",
    "        fun_eq = lambda alpha: np.dot(y, alpha)\n",
    "        jac_eq = lambda alpha: y\n",
    "        fun_ineq = lambda alpha: np.concatenate((np.zeros(N), self.C*np.ones(N)), axis=0) - np.dot(np.concatenate((-np.eye(N), np.eye(N)), axis=0), alpha)\n",
    "        jac_ineq = lambda alpha: np.concatenate((np.eye(N), -np.eye(N)), axis=0)\n",
    "\n",
    "        constraints = ({'type': 'eq',  'fun': fun_eq, 'jac': jac_eq},\n",
    "                       {'type': 'ineq',\n",
    "                        'fun': fun_ineq ,\n",
    "                        'jac': jac_ineq})\n",
    "\n",
    "        optRes = optimize.minimize(fun=lambda alpha: loss(alpha),\n",
    "                                   x0=np.ones(N),\n",
    "                                   method='SLSQP',\n",
    "                                   jac=lambda alpha: grad_loss(alpha),\n",
    "                                   constraints=constraints)\n",
    "        self.alpha = optRes.x\n",
    "        ## Assign the required attributes\n",
    "        support_indices = np.where((self.alpha+self.epsilon<self.C) * (self.alpha-self.epsilon>0))[0]\n",
    "        self.support = X[support_indices] #'''------------------- A matrix with each row corresponding to a point that falls on the margin ------------------'''\n",
    "        if len(support_indices)==0:\n",
    "            self.b = 0\n",
    "        else:\n",
    "            self.b = np.mean(y[support_indices] - np.dot(K, y*self.alpha)[support_indices]) #''' -----------------offset of the classifier------------------ '''\n",
    "        self.norm_f = np.sqrt(np.dot(y*self.alpha, np.dot(K, y*self.alpha))) # '''------------------------RKHS norm of the function f ------------------------------'''\n",
    "        self.z = y*self.alpha # we need it to define the separating function\n",
    "        self.X_train = X # we need it to define the separating function\n",
    "        self.to_fit = False\n",
    "\n",
    "    ### Implementation of the separting function $f$\n",
    "    def separating_function(self, x):\n",
    "        # Input : matrix x of shape N data points times d dimension\n",
    "        # Output: vector of size N\n",
    "        K = self.kernel(x, self.X_train)\n",
    "        return K @ self.z\n",
    "\n",
    "\n",
    "    def predict(self, x):\n",
    "        \"\"\" Predict y values in {-1, 1} \"\"\"\n",
    "        d = self.separating_function(x)\n",
    "        return 2 * (d+self.b> 0) - 1\n",
    "\n",
    "class Kernel_OvO: # One v One classification\n",
    "    def __init__(self, C, nb_classes, kernel, clf, epsilon = 1e-3):\n",
    "        self.clf = clf\n",
    "        self.C = C\n",
    "        self.epsilon = epsilon\n",
    "        self.kernel = kernel\n",
    "        self.nb_classes = nb_classes\n",
    "        self.support = None\n",
    "        self.classifiers = {}\n",
    "        self.indices = {}\n",
    "        for i in range(nb_classes):\n",
    "            for j in range(nb_classes):\n",
    "                if i!=j:\n",
    "                    if self.clf == 'SVC':\n",
    "                        self.classifiers[(i,j)] = KernelSVC(C = self.C, kernel = self.kernel, epsilon = self.epsilon)\n",
    "                    elif self.clf == 'KLR':\n",
    "                        self.classifiers[(i,j)] = KLR(lbda = self.C, kernel = self.kernel) # C here is just lbda\n",
    "                    else:\n",
    "                        print(\"--> method not recognized\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # we need to fit every classifier in self.classifiers\n",
    "        K = self.kernel(X, X)\n",
    "        self.support = X\n",
    "        for i in range(self.nb_classes):\n",
    "            for j in range(self.nb_classes):\n",
    "                if i!=j:\n",
    "                    indices_i = np.where(y==i)[0]\n",
    "                    indices_j = np.where(y==j)[0]\n",
    "                    self.indices[(i,j)] = np.concatenate((indices_i, indices_j))\n",
    "                    X_ij = X[self.indices[(i,j)], :]\n",
    "                    K_ij = K[np.ix_(self.indices[(i,j)], self.indices[(i,j)])]\n",
    "                    if self.classifiers[(i,j)].to_fit:\n",
    "                      self.classifiers[(i,j)].fit(X_ij, 2 * (y[self.indices[(i,j)]]==i) - 1)\n",
    "                    print(\"--> done\")\n",
    "\n",
    "    def predict(self, x):\n",
    "        # we need to keep having OvO until we reach the best class\n",
    "        K = self.kernel(x, self.support)\n",
    "        classes = np.zeros((x.shape[0], self.nb_classes))\n",
    "        for i in range(self.nb_classes):\n",
    "            for j in range(self.nb_classes):\n",
    "                if i!=j:\n",
    "                    y = self.classifiers[(i,j)].predict(x)\n",
    "                    indices_i = np.where(y==1)[0]\n",
    "                    indices_j = np.where(y==-1)[0]\n",
    "                    classes[indices_i, i] += 1\n",
    "                    classes[indices_j, j] += 1\n",
    "        return np.argmax(classes, axis=1)\n",
    "\n",
    "class Kernel_OvR: # One v Rest classification\n",
    "\n",
    "    def __init__(self, C, nb_classes, kernel, clf, epsilon = 1e-3):\n",
    "        self.clf = clf\n",
    "        self.C = C\n",
    "        self.kernel = kernel\n",
    "        self.epsilon = epsilon\n",
    "        self.classifiers = {}\n",
    "        self.nb_classes = nb_classes\n",
    "        self.support = None\n",
    "        for i in range(nb_classes):\n",
    "            if self.clf=='SVC':\n",
    "                self.classifiers[i] = KernelSVC(self.C, self.epsilon)\n",
    "            elif self.clf=='KLR':\n",
    "                self.classifiers[i] = KLR(self.C)\n",
    "            else:\n",
    "                print(\"--> method not recognized\")\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # we need to fit every classifier in self.classifiers\n",
    "        K = self.kernel(X, X)\n",
    "        self.support = X\n",
    "        for i in tqdm(range(self.nb_classes)):\n",
    "            y_i = np.where(y==i, 1, -1)\n",
    "            self.classifiers[i].fit(X, y_i, K)\n",
    "\n",
    "    def predict(self, x):\n",
    "        # we need to keep having OvO until we reach the best class\n",
    "        K = self.kernel(x, self.support)\n",
    "        classes = np.zeros((x.shape[0], self.nb_classes))\n",
    "        for i in range(self.nb_classes):\n",
    "            classes[:, i] = self.classifiers[i].separating_function(K) + self.classifiers[i].b\n",
    "        return np.argmax(classes, axis=1)\n",
    "\n",
    "class KMeans: # Kmeans algorithm which is used in the bag of words representation\n",
    "    def __init__(self, nb_clusters, niter=100):\n",
    "        self.nb_clusters = nb_clusters\n",
    "        self.centers = None\n",
    "        self.niter = niter\n",
    "    def fit(self, X):\n",
    "        N = X.shape[0]\n",
    "        centers = X[np.random.randint(0, N, self.nb_clusters), :]\n",
    "        for i in tqdm(range(self.niter)):\n",
    "            distances = np.sum((X - centers[:, np.newaxis, :])**2, axis=2)\n",
    "            nearest = np.argmin(distances, axis=0)\n",
    "            for k in range(self.nb_clusters):\n",
    "                cluster_data = X[nearest == k]\n",
    "                if cluster_data.shape[0] > 0:\n",
    "                    centers[k, :] = np.mean(cluster_data, axis=0)\n",
    "        self.centers = centers\n",
    "    def predict(self, x):\n",
    "        nearest = np.argmin(np.linalg.norm(x - np.expand_dims(self.centers, axis=1), axis=-1), axis=0)\n",
    "        return nearest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bec262",
   "metadata": {},
   "source": [
    "#### feature extraction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d489810a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_descriptors_sift(imgs, threshold, sigma): # sift feature extraction\n",
    "    descriptors = []\n",
    "    sift = cv2.SIFT_create(contrastThreshold=threshold, sigma=sigma)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        gray_image = to_gray(imgs[i, :])\n",
    "        _, d = sift.detectAndCompute((gray_image * 255).astype(np.uint8), None)\n",
    "        descriptors.append(d)\n",
    "    return descriptors\n",
    "\n",
    "def nearest_to_BoW(nearest, descriptors, nb_clusters):\n",
    "    BoW = np.zeros((len(descriptors), nb_clusters))\n",
    "    k = 0\n",
    "    for i in range(len(descriptors)):\n",
    "        nearest_i = nearest[k: k+descriptors[i].shape[0]]\n",
    "        indices, counts = np.unique(nearest_i, return_counts=True)\n",
    "        BoW[i, indices] += counts\n",
    "        k += descriptors[i].shape[0]\n",
    "    return BoW\n",
    "\n",
    "def compute_HOG(imgs): # HoG feature extraction\n",
    "    descriptors = []\n",
    "    hog = cv2.HOGDescriptor(_winSize=(32,32), _blockStride=(4,4), _blockSize=(16,16),\n",
    "                        _cellSize=(4,4), _nbins=9)\n",
    "    for i in range(imgs.shape[0]):\n",
    "        gray_image = to_gray(imgs[i, :])\n",
    "        features = hog.compute((gray_image * 255).astype(np.uint8))\n",
    "        descriptors.append(features.tolist())\n",
    "    return np.array(descriptors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254b0bf3",
   "metadata": {},
   "source": [
    "#### our best model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cc8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we retrieve the data\n",
    "Xtr = pd.read_csv('Xtr.csv.zip',header=None, sep=',', usecols=range(3072), compression='zip').dropna().values\n",
    "Xte = pd.read_csv('Xte.csv.zip',header=None, sep=',', usecols=range(3072), compression='zip').dropna().values\n",
    "Ytr = pd.read_csv('Ytr.csv',sep=',',usecols=[1]).squeeze().dropna().values\n",
    "# we augment the data using flips\n",
    "augmented_Xtr, augmented_Ytr = augment_data(Xtr, Ytr)\n",
    "# we extract the HoG features\n",
    "hog_tr = compute_HOG(augmented_Xtr)\n",
    "hog_te = compute_HOG(Xte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87aa207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit the classifier OvO to the HoG features on the augmented data\n",
    "scaler = StandardScaler()\n",
    "kernel = RBF(sigma=np.sqrt(hog_tr.shape[1])).kernel\n",
    "clf = Kernel_OvO(C=1., kernel=kernel, nb_classes=10, clf='SVC')\n",
    "clf.fit(scaler.fit_transform(hog_tr), augmented_Ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1cd348e",
   "metadata": {
    "id": "a1cd348e"
   },
   "source": [
    "#### submit prediction :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dafff6d",
   "metadata": {
    "id": "5dafff6d"
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict(scaler.transform(hog_te))\n",
    "Yte = {'Prediction' : prediction}\n",
    "dataframe = pd.DataFrame(Yte)\n",
    "dataframe.index += 1\n",
    "dataframe.to_csv('Yte_pred_hog_SVC_bloc_size_16_augmented_data.csv',index_label='Id')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
